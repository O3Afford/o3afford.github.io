<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="O3Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation">
  <meta name="keywords" content="Affordance Grounding, One-shot Learning, Robotic Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>O3Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">O3Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tongxuan259.github.io">Tongxuan Tian</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=sIY8FtgAAAAJ&hl=en">Xuhui Kang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yenlingkuo.com/">Yen-Ling Kuo</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Virginia</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
      
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Tongxuan259/O3Afford"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Code</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Video Presentation -->

<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <!-- Paper Video -->
          <h3 class="title is-4">Video Presentation</h3>
          <div class="content has-text-centered">
            <video id="replay-video"
                   controls
                   muted
                   preload
                   playsinline
                   width="125%">
              <source src="./static/videos/187_o3afford_video.mp4"
                      type="video/mp4">
            </video>
    
        </div>
      </div>
    </div>
  </section>

<!-- Moved teaser video section here
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation
      </h2>
    </div>
  </div>
</section> -->

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Grounding object affordance is fundamental to robotic manipulation as it establishes the critical link between perception and action among interacting objects. However, prior works predominantly focus on predicting single-object affordance, overlooking the fact that most real-world interactions involve relationships between pairs of objects. In this work, we address the challenge of object-to-object affordance grounding under limited data. Inspired by recent advances in few-shot learning with 2D vision foundation models, we propose a novel one-shot 3D object-to-object affordance learning approach for robotic manipulation. Semantic features from vision foundation models combined with point cloud representation for geometric understanding enable our one-shot learning pipeline to generalize effectively to novel objects and categories. We further integrate our 3D affordance representation with large language models (LLMs) for robotics manipulation, significantly enhancing LLMs' capability to comprehend and reason about object interactions when generating task-specific constraint functions. Our experiments on 3D object-to-object affordance grounding and robotic manipulation demonstrate that our O$^3$Afford significantly outperforms existing baselines in terms of both accuracy and generalization capability.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!-- Added Method Overview Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          
          <div class="has-text-centered">
            <!-- Add a method overview diagram/image here -->
            <img src="./static/images/framework_v5-1.png" alt="Method Overview">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Affordance Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment Results</h2>
        
        
        
        <!-- Results Carousel -->
        <h3 class="title is-4">Affordance Prediction</h3>
        <div class="has-text-centered">
            <!-- Add a method overview diagram/image here -->
            <img src="./static/images/affordance_qualitative_v5-1.png" alt="Method Overview">
        </div>

        <!-- Results Carousel -->
        <h3 class="title is-4">Robotic Manipulation</h3>
        <div class="results-carousel carousel" id="manipulation-carousel">
          <div class="item item-gif1">
            <img src="static/videos/tasks/image95.gif" alt="Hammer Task" style="width:100%; height:200px; object-fit:cover; border-radius:5px;">
          </div>
          <div class="item item-gif2">
            <img src="static/videos/tasks/image96.gif" alt="Knife Task" style="width:100%; height:200px; object-fit:cover; border-radius:5px;">
          </div>
          <div class="item item-gif3">
            <img src="static/videos/tasks/image97.gif" alt="Mugtree Task" style="width:100%; height:200px; object-fit:cover; border-radius:5px;">
          </div>
          <div class="item item-gif4">
            <img src="static/videos/tasks/image98.gif" alt="Pour Task" style="width:100%; height:200px; object-fit:cover; border-radius:5px;">
          </div>
          <div class="item item-gif5">
            <img src="static/videos/tasks/image99.gif" alt="Toaster Task" style="width:100%; height:200px; object-fit:cover; border-radius:5px;">
          </div>
          
        </div>

        
    
      </div>
    </div>
  </div>
</section>

<!-- Intra-class Generalization Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Intra-class Generalization</h2>
        
        <!-- Hammer Task -->
        <div class="results-carousel carousel" id="hammer-carousel">
          <div class="item">
            <img src="static/videos/intra-class/cut/image99.gif" alt="Hammer Generalization 1" style="width:100%; object-fit:cover; border-radius:5px;">
          </div>
          <div class="item">
            <img src="static/videos/intra-class/cut/image101.gif" alt="Hammer Generalization 2" style="width:100%; object-fit:cover; border-radius:5px;">
          </div>
        </div>
        
        <!-- Knife Task -->
        <div class="results-carousel carousel" id="knife-carousel">
          <div class="item">
            <img src="static/videos/intra-class/hang/image98.gif" alt="Knife Generalization 1" style="width:100%; object-fit:cover; border-radius:5px;">
          </div>
          <div class="item">
            <img src="static/videos/intra-class/hang/image102.gif" alt="Knife Generalization 2" style="width:100%; object-fit:cover; border-radius:5px;">
          </div>
          
        </div>
        
        <!-- Mugtree Task -->
        <div class="results-carousel carousel" id="mugtree-carousel">
          <div class="item">
            <img src="static/videos/intra-class/pour/image95.gif" alt="Mugtree Generalization 1" style="width:100%; object-fit:cover; border-radius:5px;">
          </div>
          <div class="item">
            <img src="static/videos/intra-class/pour/image100.gif" alt="Mugtree Generalization 2" style="width:100%; object-fit:cover; border-radius:5px;">
          </div>
        </div>
        
        
        
      </div>
    </div>
  </div>
</section>


<!-- Paper video moved to experiment results section -->



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{tian2025o3afford,
  author    = {Tian, Tongxuan and Kang, Xuhui and Kuo, Yen-Ling},
  title     = {O3Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation},
  journal   = {TBD},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              Website template borrowed from <a href="https://nerfies.github.io">Nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>